---
title: "p8105_hw2_geg2145"
author: "Gustavo Garcia-Franceschini"
date: "`r Sys.Date()`"
output: github_document
---

```{r setup, include=FALSE}
library(tidyverse)
library(readxl)
```

# Problem 1

First, clean the data in pols-month.csv. Use separate() to break up the variable mon into integer variables year, month, and day; replace month number with month name; create a president variable taking values gop and dem, and remove prez_dem and prez_gop; and remove the day variable.

```{r p1_pols}
df_pols = read_csv(file = "./data/pols-month.csv") %>%
  janitor::clean_names() %>%
  separate(col = mon, 
           sep= "-", 
           into = c("year", "month", "day"),
           convert = T) %>%
  mutate(month = month.name[month],
         president = ifelse(prez_dem == 1, "dem", "gop")) %>%
  select(-c("day", "prez_dem", "prez_gop"))
```

Second, clean the data in snp.csv using a similar process to the above. For consistency across datasets, arrange according to year and month, and organize so that year and month are the leading columns.

```{r p1_snp}
df_snp = read_csv(file = "./data/snp.csv") %>%
  janitor::clean_names() %>%
  separate(col = date, 
           sep= "/", 
           into = c("month", "day", "year"),
           convert = T) %>%
  mutate(month = month.name[month],
         year = ifelse(year < 50, year + 2000, year + 1900)) %>%
  arrange(year, month) %>%
  relocate(year, month)
```

Third, tidy the unemployment data so that it can be merged with the previous datasets. This process will involve switching from “wide” to “long” format; ensuring that key variables have the same name; and ensuring that key variables take the same values.

```{r p1_unemp}
df_unemp = read_csv(file = "./data/unemployment.csv") %>%
  janitor::clean_names() %>%
  pivot_longer(jan:dec,
               names_to = "month",
               values_to = "unemployment_rate") %>%
  mutate(month = case_when(
    month == "jan" ~ "January", 
    month == "feb" ~ "February", 
    month == "mar" ~ "March", 
    month == "apr" ~ "April", 
    month == "may" ~ "May", 
    month == "jun" ~ "June", 
    month == "jul" ~ "July", 
    month == "aug" ~ "August", 
    month == "sep" ~ "September", 
    month == "oct" ~ "October", 
    month == "nov" ~ "November", 
    month == "dec" ~ "December"
  )) %>%
  arrange(year, month)
```

Join the datasets by merging snp into pols, and merging unemployment into the result.

```{r p1_final_df}
df_p1 = df_pols %>%
  left_join(., df_snp, by = c("year", "month")) %>%
  left_join(., df_unemp, by = c("year", "month"))
```

Write a short paragraph about these datasets. Explain briefly what each dataset contained, and describe the resulting dataset (e.g. give the dimension, range of years, and names of key variables).

**The `pols` dataset had information on the number of politicians that were either Republican or Democrat at specific dates. In particular, we were interested in whether the president was Republican or Democrat (the `president` variable). The `snp` data contained the `close` variable, which is the closing value of the S&P stock index for specific dates. Likewise, the `unemployment` dataset had the `unemployment rate` in the US for a specific date. The resulting dataset has these variables, with the date (`year`-`month` combinations) being useful for merging. The data spans from `r min(pull(df_p1, year))` to `r max(pull(df_p1, year))`, has `r nrow(df_p1)` rows and `r ncol(df_p1)` columns.**

# Problem 2

Read and clean the Mr. Trash Wheel sheet: specify the sheet in the Excel file and to omit non-data entries (rows with notes / figures; columns containing notes) using arguments in read_excel, use reasonable variable names, omit rows that do not include dumpster-specific data.

The data include a column for the (approximate) number of homes powered. This calculation is described in the Homes powered note, but not applied to every row in the dataset. Update the data to include a new homes_powered variable based on this calculation.

```{r p2_mrtrash}
#After changing the excel file name to tash.xlsx
df_mrTrash = read_excel("./data/trash_new.xlsx", 
                        sheet = "Mr. Trash Wheel", 
                        col_names = TRUE, skip = 1,
                        range = cell_cols("A:N")) %>%
  janitor::clean_names() %>% 
  filter(!is.na(dumpster))%>%
  mutate(homes_powered = weight_tons * 500/30,
         wheel = "Mr. Trash Wheel",
         year = as.numeric(year))
```

Use a similar process to import, clean, and organize the data for Professor Trash Wheel and Gwynnda, and combine these with the Mr. Trash Wheel dataset to produce a single tidy dataset. To keep track of which Trash Wheel is which, you may need to add an additional variable to all datasets before combining.

```{r p2_prof_trash}
df_profTrash = read_excel("./data/trash_new.xlsx", 
                          sheet = "Professor Trash Wheel", 
                        col_names = TRUE, skip = 1,
                        range = cell_cols("A:M")) %>%
  janitor::clean_names() %>% 
  filter(!is.na(dumpster)) %>%
  mutate(homes_powered = weight_tons * 500/30,
         wheel = "Professor Trash Wheel")
```

```{r p2_gwynnda}
df_gwynnda = read_excel("./data/trash_new.xlsx", 
                        sheet = "Gwynnda Trash Wheel", 
                        col_names = TRUE, skip = 1,
                        range = cell_cols("A:L")) %>%
  janitor::clean_names() %>% 
  filter(!is.na(dumpster)) %>%
  mutate(homes_powered = weight_tons * 500/30,
         wheel = "Gwynnda")
```

```{r p2_merging}
df_trashWheels = bind_rows(df_mrTrash, 
                           df_profTrash, df_gwynnda)
```

Write a paragraph about these data; you are encouraged to use inline R. Be sure to note the number of observations in the resulting dataset, and give examples of key variables. For available data, what was the total weight of trash collected by Professor Trash Wheel? What was the total number of cigarette butts collected by Gwynnda in July of 2021?

**The final dataset has information on the trash collected by each of the three trash wheels (Mr. Trash Wheel, Professor Trash Wheel and Gwynnda) and put into each one of their dumpsters. Every row represent trash put by one of these trash wheels into one of their dumpsters (`r nrow(df_trashWheels)` rows total). There are `r ncol(df_trashWheels)`, among which we have the unique `dumpster`-`wheel` combination, the `weight_tons` collected, and the `homes_powered` by that specific wheel.**

**Professor Trash Wheel collected `r sum(pull(df_profTrash, weight_tons))` tons of trash total.**

**Gwynnda collected `r sum(pull(filter(df_gwynnda, month == "July" & year == 2022), cigarette_butts))` cigarette butts in July of 2022.**

# Problem 3

This problem uses data collected in an observational study to understand the trajectory of Alzheimer’s disease (AD) biomarkers. Study participants were free of Mild Cognitive Impairment (MCI), a stage between the expected cognitive decline of normal aging and the more serious decline of dementia, at the study baseline.

Basic demographic information were measured at the study baseline. The study monitored the development of MCI and recorded the age of MCI onset during the follow-up period, with the last visit marking the end of follow-up. APOE4 is a variant of the apolipoprotein E gene, significantly associated with a higher risk of developing Alzheimer’s disease. The amyloid 42/40 ratio holds significant promise for diagnosing and predicting disease outcomes. This ratio undergoes changes over time and has been linked to the manifestation of clinical symptoms of Alzheimer’s disease.

Import, clean, and tidy the dataset of baseline demographics. Ensure that sex and APOE4 carrier status are appropriate encoded (i.e. not numeric), and remove any participants who do not meet the stated inclusion criteria (i.e. no MCI at baseline). Discuss important steps in the import process and relevant features of the dataset. How many participants were recruited, and of these how many develop MCI? What is the average baseline age? What proportion of women in the study are APOE4 carriers?

```{r p3_baseline, warning=FALSE}
df_baseline = read_csv(file = "./data/MCI_baseline.csv",
                       skip = 1) %>%
  janitor::clean_names() %>%
  mutate(sex = ifelse(sex == 1, "male", "female"),
         apoe4 = ifelse(apoe4 == 1, "carrier", "not carrier"),
  age_at_onset = case_when(
           age_at_onset == "." ~ NA,
           TRUE ~ as.numeric(age_at_onset)
         )
         ) %>%
  filter(is.na(age_at_onset) | current_age < age_at_onset)
```

**After importing the data with a standard `read_csv` call, I noticed the first row described each column, so I had to skip that. I cleaned all the variable names and changed the values of `sex` and `apoe4` to "male"/"female" and "carrier"/"not carrier", respectively. Since I wanted the  `age_on_onset` variable to be numerical, I exchanged all "." for NAs. Finally, I filtered out individuals who's `age_on_onset` was lower than their `current_age` (meaning they already had MCI when the study started). After all that data cleaning, we had `r nrow(df_baseline)` total individuals recruited, out of which `r nrow(filter(df_baseline, !is.na(age_at_onset)))` developed MCI. The average baseline age was `r mean(pull(df_baseline, current_age))` and `r nrow(filter(df_baseline, sex == "female" & apoe4 == "carrier"))/nrow(filter(df_baseline, sex == "female")) * 100` percent of women are APOE4 carriers.**

Similarly, import, clean, and tidy the dataset of longitudinally observed biomarker values; comment on the steps on the import process and the features of the dataset.

```{r p3_long}
df_long = read_csv(file = "./data/mci_amyloid.csv",
                       skip = 1) %>%
  janitor::clean_names() %>%
  pivot_longer("baseline":"time_8",
               names_to= "years_since_baseline",
               values_to= "biomarker_value") %>%
  mutate(years_since_baseline = factor(case_when(
    years_since_baseline == "baseline" ~ "baseline",
    years_since_baseline == "Na" ~ NA,
    TRUE ~ str_c(str_sub(
      years_since_baseline,-1,-1), " years"))),
    biomarker_value = case_when(
      years_since_baseline == "Na" ~ NA,
      TRUE ~ as.numeric(biomarker_value)
    )) %>%
  rename(id = study_id)
```

**The dataset was imported using `read_csv` and skipping the first row (since it contained descriptions of the data). After cleaning the data, we have three variables: the individual's `id` and their `biomarker_value` for a specific time in the study (baseline, 2 years, 4 years, 6 years, 8 years). Note we changed the id variable name from `study_id` to `id` to facilitate future merging. Some data cleaning also took place so that `biomarker_value` is can be numeric variable and `years_since_baseline` can be a factor variable with 5 levels.**

Check whether some participants appear in only the baseline or amyloid datasets, and comment on your findings. Combine the demographic and biomarker datasets so that only participants who appear in both datasets are retained, and briefly describe the resulting dataset; export the result as a CSV to your data directory.

```{r p3_merging}
df_unmatched_base = anti_join(df_baseline, df_long, by = "id")

df_unmatched_long = anti_join(df_long, df_baseline, by = "id")

df_mci_study = inner_join(df_long, df_baseline, by = "id")

write_csv(df_mci_study, "./data/df_mci_study.csv")
```

**There are `r nrow(df_unmatched_base)` individuals in the baseline dataset that we don't have longitudinal data for (ID's `r unique(pull(df_unmatched_base, id))`) and `r nrow(df_unmatched_long)` individuals in the longitudinal dataset that we don't have baseline data for (ID's `r unique(pull(df_unmatched_long, id))`).**

**DESCRIBE FINAL DATASET**
